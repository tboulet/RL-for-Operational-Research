# name: Monte Carlo
name : Monte Carlo
config:
  
  # ============== Hyperparameters ==============
  # The learning rate
  learning_rate: 0.1

  # The discount factor
  gamma : 0.99

  # The number of steps used in the estimation of the Q values.
  # When reaching step i+n_step of the episode, the Q value of the state at step i will be updated using the transitions i to i+n_step-1 in memory.
  # Can be 'inf'. In that case, the algorithm will wait until the end of the episode to update the Q values.
  n_steps : 30

  # ============== Q Values Model ==============
  q_model:
    class_string: "src.learners.tabular:QValuesTabularLearner"
    method_q_value_initialization : random
    typical_return : 1.0
    typical_return_std : 1.0

  # ============== Exploration ==============
  # The method for the exploratory policy. Either "eps_greedy", "boltzmann", "ucb",
  method_exploration : eps_greedy

  # The UCB constant (if method_exploration is UCB)
  ucb_constant : 2.0

  # The boltzmann temperature (if method_exploration is boltzmann)
  boltzmann_temperature:
    class_string : "src.schedulers:Exponential"
    start_value : 10.0
    end_value : 0.01
    n_steps : ${n_steps_exploration}

  # The epsilon parameter (if method_exploration is eps_greedy)
  # epsilon : 0.1
  epsilon:
    class_string : "src.schedulers:Exponential"
    start_value : 1.0
    end_value : 0.05
    n_steps : ${n_steps_exploration}
    lower_bound : 0.05
  
  # ============== Logging ==============
  do_log_q_values : False
  n_max_q_values_states_to_log : 1
  do_log_actions_chosen : False
